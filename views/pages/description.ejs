<% include ../partials/head %>
<% include ../partials/header %>

<section>
  <div class="container container-twelve">
    <!--<div class="row">
      <div class="twelve columns">

      </div>
    </div>-->
    <div class="row">
      <div class="eight columns">
      	<h2>Project description</h2>
        <div class="panel cms-content">
            <h3 id="whysewa">Why SEWA?</h3>
        <p>Technologies that can robustly and accurately analyse human facial, vocal and verbal behaviour and
           interactions in the wild, as observed by omnipresent webcams in digital devices, would have profound
           impact on both basic sciences and the industrial sector. They could open up tremendous potential to measure
           behaviour indicators that heretofore resisted measurement because they were too subtle or fleeting to be
           measured by the human eye and ear, would effectively lead to development of the next generation of
           efficient, seamless and user-centric human-computer interaction (affective multimodal interfaces, interactive
           multi-party games, and online services), would have profound impact on business (automatic market
           research analysis would become possible, recruitment would become green as travels would be reduced
           drastically), would enable next generation healthcare technologies (remote monitoring of conditions like
           pain, anxiety and depression), to mention but a few examples.</p>
            <h3 id="howsewaworks">How SEWA works?</h3>
        <p>The overall aim of the SEWA project is to enable such technology, i.e., to capitalise on existing state-of-
           the-art methodologies, models and algorithms for machine analysis of facial, vocal and verbal behaviour, and
           then adjust and combine them to realise naturalistic human-centric human-computer interaction (HCI) and
           computer-mediated face-to-face interaction (FF-HCI) for data recorded by a device as cheap as a web-cam
           and in almost arbitrary recording conditions including semi-dark, dark and noisy rooms with dynamic change
           of room impulse response and distance to sensors. This extends and contrasts considerably the current state
           of the art in existing technological solutions to machine analysis of the facial, vocal and verbal behaviour
           that are used in (commercially and otherwise) available human-centric HCI and FF-HCI applications.</p>
        <p>To wit, shortcomings of existing technologies for automatic analysis of human behaviour are numerous.</p>
        <ul>
            <li>Current studies have been largely conducted in laboratory conditions so far with controlled noise
                level, reverberation, often limited verbal content, illumination, calibrated cameras, and subjects who
                are instructed not to eat or talk on the phone while being recorded. Such conditions are very difficult
                to reproduce in real-world applications and tools trained on such data usually do not generalise well
                to behavioural recordings made in the wild (in unconstrained settings typical for HCI and FF-HCI
                scenarios).</li>
            <li>Interpretation of facial and vocal behaviour depends crucially on the dynamics of the behaviour
                (timing, velocity, frequency, temporal inter-dependencies between gestures), which is currently not
                taken into account.</li>
            <li>Observed behaviours may be influenced by those of an interlocutor and thus require analysis of both
                interactants, especially to measure such critically important patterns as mimicry, rapport, and
                sentiment, in general, but this is not currently taken into account. Existing approaches typically
                perform analysis of a single individual and FF-HCI is not addressed as a problem of simultaneous
                analysis of both interacting parties.</li>
        </ul>
            <p>The main aim of the SEWA project is to address these shortcomings of the current HCI and FF-HCI
                technology and develop novel, robust technology for machine analysis of facial, vocal and verbal behaviour
                <em>in the wild</em> as shown by a single person or by two (or more) interactants.</p>
            <h3 id="whereissewaapplied">Where is SEWA applied?</h3>

        <p>As a proof of concept, and with the focus on novel HCI and FF-HCI applications, SEWA technology will
           be applied to:</p>
        <ul>
            <li>machine inference of <em>sentiment/</em> liking ratings in response to multimedia content (movie trailers,
                product adverts, etc.) watched by people in the wild, based on which a <em>multimedia recommender</em>
                system will be built, offering to the user multimedia content being similar to that she liked
                previously;</li>
            <li>automatic estimation of <em>sentiment, rapport and empathy</em> shown by two people involved in unscripted
                computer-mediated dyadic interaction (using an online video chat service like opentok), based on
                which <em>Chat Roulette Social Game</em> will be developed that tries to find out people with whom one
                would like to chat (the underlying phenomena are the formation and dynamics of clusters of people
                that like to chat with each other, e.g., have the same opinions about debating issues; based on which
                formation and dynamics of social media/networks and opinion polls can be studied, and serious
                games providing depth of learning and participation can be built).</li>
        </ul>
        </div>
      </div>
      <div class="four columns">
        <% include ../partials/workshops_side %>
        <div class="panel boxed">
			<h3>Participants</h3>
            <div class="textcenter">
            	<p><a href="http://www.imperial.ac.uk/" target="_blank"><img src="images/logo_imperial_college_london.png" alt="Imperial College London" class="responsive grayscale"></a></p>
                <p><a href="http://www.uni-passau.de/en/" target="_blank"><img src="images/passau.png" alt="University of Passau" class="responsive grayscale"></a></p>
                <p><a href="http://www.realeyesit.com/" target="_blank"><img src="images/realeyes.png" alt="Reale Eyes" class="responsive grayscale"></a></p>
                <p><a href="http://playgen.com/" target="_blank"><img src="images/Playgen.png" alt="Playgen" class="responsive grayscale"></a></p>
            </div>
        </div>
      	<div class="panel boxed">
			<h3>Sponsors</h3>
            <div class="textcenter">
            <p><img src="images/eu-logo.png" alt="sponsors" class="responsive"><img src="images/horizon_20201.jpg" alt="sponsors" class="responsive"></p>
            <p><small>Funding from the European Commision Horizon 2020 Programme.</small></p>
            </div>
        </div>
      </div>
    </div>
  </div>
  <!-- container -->
</section>

<% include ../partials/footer %>